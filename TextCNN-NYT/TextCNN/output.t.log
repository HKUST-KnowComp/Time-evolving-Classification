Run [t/all]:
Fri, 21 Jul 2017 16:44:27 +0800 RUN TIME 2
Fri, 21 Jul 2017 16:44:27 +0800 Start train...
TRAIN_PID:7895
Fri, 21 Jul 2017 16:44:27 +0800 Start eval...
EVAL_PID:7900
Fri, 21 Jul 2017 16:44:27 +0800 waiting train & eval: 7895, 7900
whether to open up a new training:(y/n)please choose times to train:whether to train all historical data:(y/n)the evaluate data has already exists!
whether to delete the old evaluate directory:(y/n)please choose times to eval:time 2: 11398
generating train data...
training...
2017-07-21 16:44:50.742311: step=10(10/1), loss=20.3428, acc=0.2656; 0.150 sec/batch)
2017-07-21 16:44:52.247265: step=20(20/1), loss=12.2232, acc=0.4375; 0.177 sec/batch)
2017-07-21 16:44:53.722399: step=30(30/1), loss=11.0337, acc=0.5000; 0.160 sec/batch)
2017-07-21 16:44:55.024479: step=40(40/1), loss=6.8294, acc=0.5781; 0.103 sec/batch)
2017-07-21 16:44:56.044594: step=50(50/1), loss=6.3776, acc=0.5625; 0.116 sec/batch)
2017-07-21 16:44:57.459610: step=60(60/1), loss=5.2834, acc=0.6875; 0.110 sec/batch)
2017-07-21 16:44:58.885105: step=70(70/1), loss=6.2422, acc=0.5938; 0.144 sec/batch)
2017-07-21 16:45:00.380584: step=80(80/1), loss=7.5153, acc=0.5625; 0.132 sec/batch)
2017-07-21 16:45:01.904366: step=90(90/1), loss=4.3650, acc=0.7344; 0.160 sec/batch)
2017-07-21 16:45:03.448053: step=100(100/1), loss=6.2658, acc=0.7188; 0.167 sec/batch)
2017-07-21 16:45:04.727813: step=110(110/1), loss=3.2322, acc=0.7812; 0.073 sec/batch)
2017-07-21 16:45:06.723096: step=120(120/1), loss=3.5789, acc=0.7031; 0.163 sec/batch)
2017-07-21 16:45:08.382425: step=130(130/1), loss=5.8131, acc=0.6406; 0.136 sec/batch)
2017-07-21 16:45:09.944408: step=140(140/1), loss=2.8626, acc=0.7656; 0.168 sec/batch)
2017-07-21 16:45:11.834871: step=150(150/1), loss=3.2194, acc=0.7812; 0.222 sec/batch)
2017-07-21 16:45:13.762097: step=160(160/1), loss=5.6966, acc=0.7344; 0.161 sec/batch)
2017-07-21 16:45:15.285404: step=170(170/1), loss=4.3141, acc=0.7188; 0.174 sec/batch)
2017-07-21 16:45:17.246016: step=180(1/2), loss=4.6258, acc=0.7500; 0.193 sec/batch)
2017-07-21 16:45:18.430548: step=190(11/2), loss=3.3756, acc=0.8281; 0.080 sec/batch)
2017-07-21 16:45:19.429287: step=200(21/2), loss=0.9401, acc=0.8906; 0.098 sec/batch)
2017-07-21 16:45:20.444632: step=210(31/2), loss=1.7382, acc=0.8438; 0.099 sec/batch)
2017-07-21 16:45:21.448995: step=220(41/2), loss=4.0749, acc=0.7188; 0.100 sec/batch)
2017-07-21 16:45:22.639482: step=230(51/2), loss=2.6230, acc=0.8281; 0.162 sec/batch)
2017-07-21 16:45:24.309719: step=240(61/2), loss=3.9180, acc=0.7656; 0.198 sec/batch)
2017-07-21 16:45:26.239559: step=250(71/2), loss=2.4877, acc=0.8125; 0.177 sec/batch)
2017-07-21 16:45:28.300317: step=260(81/2), loss=1.8987, acc=0.8281; 0.205 sec/batch)
2017-07-21 16:45:30.357500: step=270(91/2), loss=1.5335, acc=0.8281; 0.180 sec/batch)
2017-07-21 16:45:31.924217: step=280(101/2), loss=1.3705, acc=0.8594; 0.152 sec/batch)
2017-07-21 16:45:33.392258: step=290(111/2), loss=3.3146, acc=0.7656; 0.158 sec/batch)
2017-07-21 16:45:34.780866: step=300(121/2), loss=3.0156, acc=0.7656; 0.100 sec/batch)
2017-07-21 16:45:36.038059: step=310(131/2), loss=2.6539, acc=0.8125; 0.161 sec/batch)
2017-07-21 16:45:37.608953: step=320(141/2), loss=1.6598, acc=0.8438; 0.166 sec/batch)
2017-07-21 16:45:39.051562: step=330(151/2), loss=2.3142, acc=0.7969; 0.140 sec/batch)
2017-07-21 16:45:40.624999: step=340(161/2), loss=2.6060, acc=0.8125; 0.135 sec/batch)
2017-07-21 16:45:42.108589: step=350(171/2), loss=3.3056, acc=0.7812; 0.151 sec/batch)
2017-07-21 16:45:43.498242: step=360(2/3), loss=2.9365, acc=0.8438; 0.141 sec/batch)
2017-07-21 16:45:45.105030: step=370(12/3), loss=2.6469, acc=0.8438; 0.183 sec/batch)
2017-07-21 16:45:46.746959: step=380(22/3), loss=0.6666, acc=0.8750; 0.165 sec/batch)
2017-07-21 16:45:48.646129: step=390(32/3), loss=1.1631, acc=0.8594; 0.266 sec/batch)
2017-07-21 16:45:50.701148: step=400(42/3), loss=1.9090, acc=0.7969; 0.192 sec/batch)
2017-07-21 16:45:52.453147: step=410(52/3), loss=1.0966, acc=0.8906; 0.122 sec/batch)
2017-07-21 16:45:54.610238: step=420(62/3), loss=1.7721, acc=0.8438; 0.135 sec/batch)
2017-07-21 16:45:55.985345: step=430(72/3), loss=2.4112, acc=0.8281; 0.122 sec/batch)
2017-07-21 16:45:56.978129: step=440(82/3), loss=2.6586, acc=0.8438; 0.101 sec/batch)
2017-07-21 16:45:58.000846: step=450(92/3), loss=2.8648, acc=0.7656; 0.088 sec/batch)
2017-07-21 16:45:59.057430: step=460(102/3), loss=2.0853, acc=0.8281; 0.092 sec/batch)
2017-07-21 16:46:00.174596: step=470(112/3), loss=0.6288, acc=0.9375; 0.137 sec/batch)
2017-07-21 16:46:02.161175: step=480(122/3), loss=2.3666, acc=0.8750; 0.110 sec/batch)
2017-07-21 16:46:03.789865: step=490(132/3), loss=4.0311, acc=0.7500; 0.199 sec/batch)
2017-07-21 16:46:05.758014: step=500(142/3), loss=1.9229, acc=0.8594; 0.179 sec/batch)
2017-07-21 16:46:07.669973: step=510(152/3), loss=2.4933, acc=0.7812; 0.235 sec/batch)
2017-07-21 16:46:09.307437: step=520(162/3), loss=2.8138, acc=0.8281; 0.176 sec/batch)
2017-07-21 16:46:10.887144: step=530(172/3), loss=0.7999, acc=0.9062; 0.184 sec/batch)
2017-07-21 16:46:12.477659: step=540(3/4), loss=0.8348, acc=0.8906; 0.085 sec/batch)
2017-07-21 16:46:13.893427: step=550(13/4), loss=1.6499, acc=0.8125; 0.126 sec/batch)
2017-07-21 16:46:15.392947: step=560(23/4), loss=1.1200, acc=0.8750; 0.119 sec/batch)
2017-07-21 16:46:16.847537: step=570(33/4), loss=2.2176, acc=0.8125; 0.162 sec/batch)
2017-07-21 16:46:18.346867: step=580(43/4), loss=1.9037, acc=0.8438; 0.126 sec/batch)
2017-07-21 16:46:19.854635: step=590(53/4), loss=0.3552, acc=0.9062; 0.161 sec/batch)
2017-07-21 16:46:21.324828: step=600(63/4), loss=2.7081, acc=0.8125; 0.104 sec/batch)
2017-07-21 16:46:22.024379: step=610(73/4), loss=1.9880, acc=0.8750; 0.060 sec/batch)
2017-07-21 16:46:23.361258: step=620(83/4), loss=2.1717, acc=0.8594; 0.123 sec/batch)
2017-07-21 16:46:25.152057: step=630(93/4), loss=1.6032, acc=0.9219; 0.180 sec/batch)
2017-07-21 16:46:27.256408: step=640(103/4), loss=0.9112, acc=0.8750; 0.234 sec/batch)
2017-07-21 16:46:30.324244: step=650(113/4), loss=1.5832, acc=0.7969; 0.209 sec/batch)
2017-07-21 16:46:32.111363: step=660(123/4), loss=0.9216, acc=0.8750; 0.118 sec/batch)
2017-07-21 16:46:33.487748: step=670(133/4), loss=1.3751, acc=0.8125; 0.101 sec/batch)
2017-07-21 16:46:34.541190: step=680(143/4), loss=1.6233, acc=0.8906; 0.103 sec/batch)
2017-07-21 16:46:35.544515: step=690(153/4), loss=1.2024, acc=0.8750; 0.105 sec/batch)
2017-07-21 16:46:36.593538: step=700(163/4), loss=0.8819, acc=0.9062; 0.090 sec/batch)
2017-07-21 16:46:38.321481: step=710(173/4), loss=1.0181, acc=0.9219; 0.184 sec/batch)
2017-07-21 16:46:39.715084: step=720(4/5), loss=0.6271, acc=0.9062; 0.138 sec/batch)
2017-07-21 16:46:41.249745: step=730(14/5), loss=1.1523, acc=0.8906; 0.138 sec/batch)
2017-07-21 16:46:42.636747: step=740(24/5), loss=0.9725, acc=0.9062; 0.189 sec/batch)
2017-07-21 16:46:44.582645: step=750(34/5), loss=1.4592, acc=0.8750; 0.144 sec/batch)
2017-07-21 16:46:46.095273: step=760(44/5), loss=0.7263, acc=0.9375; 0.146 sec/batch)
2017-07-21 16:46:48.005425: step=770(54/5), loss=0.8475, acc=0.9531; 0.147 sec/batch)
2017-07-21 16:46:49.543407: step=780(64/5), loss=1.0120, acc=0.9062; 0.157 sec/batch)
2017-07-21 16:46:51.409372: step=790(74/5), loss=0.9716, acc=0.9219; 0.147 sec/batch)
2017-07-21 16:46:52.910521: step=800(84/5), loss=0.6724, acc=0.8906; 0.128 sec/batch)
2017-07-21 16:46:54.516151: step=810(94/5), loss=1.1521, acc=0.8594; 0.137 sec/batch)
2017-07-21 16:46:56.069908: step=820(104/5), loss=1.0977, acc=0.8281; 0.146 sec/batch)
2017-07-21 16:46:57.823055: step=830(114/5), loss=0.5754, acc=0.9219; 0.106 sec/batch)
2017-07-21 16:46:58.767681: step=840(124/5), loss=0.9375, acc=0.8906; 0.097 sec/batch)
2017-07-21 16:46:59.792537: step=850(134/5), loss=0.8531, acc=0.8750; 0.100 sec/batch)
2017-07-21 16:47:00.630353: step=860(144/5), loss=1.1211, acc=0.9062; 0.056 sec/batch)
2017-07-21 16:47:02.079638: step=870(154/5), loss=2.2141, acc=0.8906; 0.228 sec/batch)
2017-07-21 16:47:04.194284: step=880(164/5), loss=1.1152, acc=0.9062; 0.179 sec/batch)
2017-07-21 16:47:06.966163: step=890(174/5), loss=0.8857, acc=0.9219; 0.228 sec/batch)
2017-07-21 16:47:08.840279: step=900(5/6), loss=1.0947, acc=0.8750; 0.191 sec/batch)
2017-07-21 16:47:10.749557: step=910(15/6), loss=0.5635, acc=0.9375; 0.138 sec/batch)
2017-07-21 16:47:11.787072: step=920(25/6), loss=1.0734, acc=0.9062; 0.101 sec/batch)
2017-07-21 16:47:12.784364: step=930(35/6), loss=1.6321, acc=0.8750; 0.103 sec/batch)
2017-07-21 16:47:13.838639: step=940(45/6), loss=1.2103, acc=0.8906; 0.106 sec/batch)
2017-07-21 16:47:15.490168: step=950(55/6), loss=0.2436, acc=0.9531; 0.168 sec/batch)
2017-07-21 16:47:17.058576: step=960(65/6), loss=1.0927, acc=0.8906; 0.172 sec/batch)
2017-07-21 16:47:18.599438: step=970(75/6), loss=0.6198, acc=0.9531; 0.167 sec/batch)
2017-07-21 16:47:20.090885: step=980(85/6), loss=1.4044, acc=0.8750; 0.166 sec/batch)
2017-07-21 16:47:21.285167: step=990(95/6), loss=0.7097, acc=0.9375; 0.143 sec/batch)
2017-07-21 16:47:22.928952: step=1000(105/6), loss=0.4049, acc=0.9375; 0.130 sec/batch)
2017-07-21 16:47:24.988078: step=1010(115/6), loss=1.1992, acc=0.9219; 0.134 sec/batch)
2017-07-21 16:47:26.519470: step=1020(125/6), loss=1.2500, acc=0.9062; 0.128 sec/batch)
2017-07-21 16:47:28.390937: step=1030(135/6), loss=0.7237, acc=0.8906; 0.178 sec/batch)
2017-07-21 16:47:30.283204: step=1040(145/6), loss=0.9178, acc=0.9062; 0.173 sec/batch)
2017-07-21 16:47:31.733286: step=1050(155/6), loss=1.3281, acc=0.8906; 0.181 sec/batch)
2017-07-21 16:47:33.263255: step=1060(165/6), loss=1.0008, acc=0.8906; 0.174 sec/batch)
2017-07-21 16:47:35.156654: step=1070(175/6), loss=0.9024, acc=0.9062; 0.108 sec/batch)
2017-07-21 16:47:36.137131: step=1080(6/7), loss=0.5118, acc=0.9375; 0.100 sec/batch)
2017-07-21 16:47:37.190038: step=1090(16/7), loss=0.6583, acc=0.9219; 0.102 sec/batch)
2017-07-21 16:47:38.185656: step=1100(26/7), loss=0.7926, acc=0.8750; 0.102 sec/batch)
2017-07-21 16:47:39.241994: step=1110(36/7), loss=1.6371, acc=0.8594; 0.143 sec/batch)
2017-07-21 16:47:40.953730: step=1120(46/7), loss=0.5064, acc=0.9219; 0.209 sec/batch)
2017-07-21 16:47:43.746797: step=1130(56/7), loss=0.4541, acc=0.9531; 0.194 sec/batch)
2017-07-21 16:47:45.697423: step=1140(66/7), loss=0.4466, acc=0.9375; 0.201 sec/batch)
2017-07-21 16:47:47.756893: step=1150(76/7), loss=1.0504, acc=0.9219; 0.156 sec/batch)
2017-07-21 16:47:49.301569: step=1160(86/7), loss=0.7445, acc=0.8750; 0.140 sec/batch)
2017-07-21 16:47:50.386022: step=1170(96/7), loss=0.6412, acc=0.9688; 0.102 sec/batch)
2017-07-21 16:47:51.477048: step=1180(106/7), loss=1.0050, acc=0.9062; 0.110 sec/batch)
2017-07-21 16:47:53.342537: step=1190(116/7), loss=1.2061, acc=0.8906; 0.170 sec/batch)
2017-07-21 16:47:54.870259: step=1200(126/7), loss=1.7330, acc=0.9062; 0.117 sec/batch)
2017-07-21 16:47:56.500840: step=1210(136/7), loss=0.8388, acc=0.8750; 0.159 sec/batch)
2017-07-21 16:47:58.071141: step=1220(146/7), loss=1.4875, acc=0.8750; 0.167 sec/batch)
2017-07-21 16:47:59.385896: step=1230(156/7), loss=0.8025, acc=0.8750; 0.058 sec/batch)
2017-07-21 16:48:00.703981: step=1240(166/7), loss=0.7258, acc=0.9375; 0.103 sec/batch)
2017-07-21 16:48:02.237313: step=1250(176/7), loss=0.5463, acc=0.8750; 0.147 sec/batch)
2017-07-21 16:48:03.646221: step=1260(7/8), loss=0.3735, acc=0.9219; 0.133 sec/batch)
2017-07-21 16:48:05.693538: step=1270(17/8), loss=0.5928, acc=0.8906; 0.225 sec/batch)
2017-07-21 16:48:07.742640: step=1280(27/8), loss=0.3983, acc=0.9219; 0.190 sec/batch)
2017-07-21 16:48:09.657062: step=1290(37/8), loss=0.7865, acc=0.9219; 0.184 sec/batch)
2017-07-21 16:48:11.739619: step=1300(47/8), loss=0.0370, acc=0.9844; 0.175 sec/batch)
2017-07-21 16:48:12.922321: step=1310(57/8), loss=0.5572, acc=0.9531; 0.112 sec/batch)
2017-07-21 16:48:13.925288: step=1320(67/8), loss=0.1578, acc=0.9375; 0.089 sec/batch)
2017-07-21 16:48:15.011517: step=1330(77/8), loss=1.0562, acc=0.9219; 0.110 sec/batch)
2017-07-21 16:48:16.016813: step=1340(87/8), loss=1.1303, acc=0.8906; 0.092 sec/batch)
2017-07-21 16:48:17.245856: step=1350(97/8), loss=1.2291, acc=0.9062; 0.154 sec/batch)
2017-07-21 16:48:19.341452: step=1360(107/8), loss=0.7994, acc=0.9219; 0.172 sec/batch)
2017-07-21 16:48:21.456997: step=1370(117/8), loss=0.7988, acc=0.8750; 0.185 sec/batch)
2017-07-21 16:48:23.388233: step=1380(127/8), loss=0.1081, acc=0.9688; 0.193 sec/batch)
2017-07-21 16:48:25.120320: step=1390(137/8), loss=0.4754, acc=0.8906; 0.137 sec/batch)
2017-07-21 16:48:26.638331: step=1400(147/8), loss=1.7985, acc=0.8594; 0.152 sec/batch)
2017-07-21 16:48:28.099109: step=1410(157/8), loss=0.6250, acc=0.9375; 0.100 sec/batch)
2017-07-21 16:48:29.681524: step=1420(167/8), loss=0.8160, acc=0.8906; 0.177 sec/batch)
2017-07-21 16:48:31.297673: step=1430(177/8), loss=0.5167, acc=0.9062; 0.193 sec/batch)
2017-07-21 16:48:32.684771: step=1440(8/9), loss=0.3847, acc=0.9219; 0.129 sec/batch)
2017-07-21 16:48:34.213236: step=1450(18/9), loss=0.7337, acc=0.9219; 0.155 sec/batch)
2017-07-21 16:48:35.810310: step=1460(28/9), loss=0.8114, acc=0.8906; 0.123 sec/batch)
2017-07-21 16:48:37.060662: step=1470(38/9), loss=0.2090, acc=0.9531; 0.117 sec/batch)
2017-07-21 16:48:38.328917: step=1480(48/9), loss=1.0789, acc=0.9062; 0.059 sec/batch)
2017-07-21 16:48:39.734375: step=1490(58/9), loss=0.1466, acc=0.9531; 0.141 sec/batch)
2017-07-21 16:48:41.151268: step=1500(68/9), loss=0.5869, acc=0.8906; 0.145 sec/batch)
2017-07-21 16:48:43.280122: step=1510(78/9), loss=0.6846, acc=0.9219; 0.206 sec/batch)
2017-07-21 16:48:45.215338: step=1520(88/9), loss=0.5763, acc=0.9219; 0.218 sec/batch)
2017-07-21 16:48:47.248027: step=1530(98/9), loss=0.7050, acc=0.9375; 0.197 sec/batch)
2017-07-21 16:48:49.534979: step=1540(108/9), loss=0.6946, acc=0.8750; 0.137 sec/batch)
2017-07-21 16:48:50.699409: step=1550(118/9), loss=0.5308, acc=0.9375; 0.118 sec/batch)
2017-07-21 16:48:51.691408: step=1560(128/9), loss=1.0069, acc=0.8594; 0.089 sec/batch)
2017-07-21 16:48:52.746909: step=1570(138/9), loss=0.4932, acc=0.9531; 0.123 sec/batch)
2017-07-21 16:48:53.720607: step=1580(148/9), loss=0.1260, acc=0.9375; 0.099 sec/batch)
2017-07-21 16:48:55.012049: step=1590(158/9), loss=0.9052, acc=0.9062; 0.170 sec/batch)
2017-07-21 16:48:57.120433: step=1600(168/9), loss=0.1898, acc=0.9531; 0.199 sec/batch)
2017-07-21 16:48:58.369491: step=1610(178/9), loss=0.4925, acc=0.9531; 0.163 sec/batch)
2017-07-21 16:49:00.157582: step=1620(9/10), loss=0.4550, acc=0.9531; 0.173 sec/batch)
2017-07-21 16:49:02.197953: step=1630(19/10), loss=0.3146, acc=0.9688; 0.172 sec/batch)
2017-07-21 16:49:03.768676: step=1640(29/10), loss=0.3431, acc=0.9531; 0.171 sec/batch)
2017-07-21 16:49:05.249244: step=1650(39/10), loss=1.2273, acc=0.8281; 0.154 sec/batch)
2017-07-21 16:49:07.322296: step=1660(49/10), loss=0.5522, acc=0.9062; 0.174 sec/batch)
2017-07-21 16:49:08.928426: step=1670(59/10), loss=0.2440, acc=0.9688; 0.226 sec/batch)
2017-07-21 16:49:10.386338: step=1680(69/10), loss=0.0416, acc=0.9844; 0.139 sec/batch)
2017-07-21 16:49:12.052851: step=1690(79/10), loss=0.1518, acc=0.9688; 0.171 sec/batch)
2017-07-21 16:49:13.559973: step=1700(89/10), loss=0.4593, acc=0.9219; 0.168 sec/batch)
2017-07-21 16:49:14.641679: step=1710(99/10), loss=1.3534, acc=0.8750; 0.092 sec/batch)
2017-07-21 16:49:16.105188: step=1720(109/10), loss=0.6783, acc=0.9375; 0.103 sec/batch)
2017-07-21 16:49:16.935839: step=1730(119/10), loss=0.4204, acc=0.9219; 0.057 sec/batch)
2017-07-21 16:49:17.806781: step=1740(129/10), loss=0.2764, acc=0.9531; 0.173 sec/batch)
2017-07-21 16:49:19.753194: step=1750(139/10), loss=0.7106, acc=0.9219; 0.210 sec/batch)
2017-07-21 16:49:21.724658: step=1760(149/10), loss=0.7397, acc=0.8906; 0.165 sec/batch)
2017-07-21 16:49:23.624351: step=1770(159/10), loss=0.3656, acc=0.9688; 0.195 sec/batch)
2017-07-21 16:49:26.492695: step=1780(169/10), loss=0.5338, acc=0.9375; 0.167 sec/batch)
2017-07-21 16:49:27.890325: step=1790(0/11), loss=0.1792, acc=0.8333; 0.015 sec/batch)
2017-07-21 16:49:28.946020: step=1800(10/11), loss=0.8621, acc=0.9219; 0.105 sec/batch)
2017-07-21 16:49:30.015235: step=1810(20/11), loss=0.1437, acc=0.9531; 0.099 sec/batch)
2017-07-21 16:49:31.027790: step=1820(30/11), loss=0.3465, acc=0.9844; 0.121 sec/batch)
2017-07-21 16:49:33.011718: step=1830(40/11), loss=0.4819, acc=0.9219; 0.133 sec/batch)
2017-07-21 16:49:34.561225: step=1840(50/11), loss=0.6555, acc=0.9219; 0.141 sec/batch)
2017-07-21 16:49:36.014737: step=1850(60/11), loss=0.5575, acc=0.9219; 0.149 sec/batch)
2017-07-21 16:49:37.431980: step=1860(70/11), loss=0.3445, acc=0.9062; 0.120 sec/batch)
2017-07-21 16:49:38.766064: step=1870(80/11), loss=0.4949, acc=0.9219; 0.135 sec/batch)
2017-07-21 16:49:40.269236: step=1880(90/11), loss=0.9175, acc=0.9219; 0.124 sec/batch)
2017-07-21 16:49:42.466250: step=1890(100/11), loss=0.9695, acc=0.9062; 0.164 sec/batch)